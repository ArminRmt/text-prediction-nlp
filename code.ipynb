{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Reading the CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def read_texts_from_csv(path, text_col_name):\n",
    "    texts = []\n",
    "    with open(path, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            texts.append(row[text_col_name])\n",
    "    return texts\n",
    "\n",
    "train_texts = read_texts_from_csv('train.csv', 'Text')\n",
    "val_texts   = read_texts_from_csv('val.csv',   'Text')\n",
    "test_texts  = read_texts_from_csv('test.csv',  'Text')\n",
    "sample_texts= read_texts_from_csv('sample.csv','Truncated Text')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Tokenization and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(sentence):\n",
    "    return sentence.strip().split()\n",
    "\n",
    "def add_start_end_tokens(token_lists):\n",
    "    out = []\n",
    "    for tok_list in token_lists:\n",
    "        out.append(['<s>'] + tok_list + ['</s>'])\n",
    "    return out\n",
    "\n",
    "# Convert train_texts -> train_raw_tokens\n",
    "train_raw_tokens = []\n",
    "for txt in train_texts:\n",
    "    raw = simple_tokenize(txt)\n",
    "    train_raw_tokens.append(raw)\n",
    "train_raw_tokens = add_start_end_tokens(train_raw_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Vocabulary that includes <unk>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 34388\n"
     ]
    }
   ],
   "source": [
    "def build_vocabulary(train_tokens, min_count=2):\n",
    "    freq = defaultdict(int)\n",
    "    for sent in train_tokens:\n",
    "        for w in sent:\n",
    "            freq[w] += 1\n",
    "\n",
    "    vocab = ['<unk>']\n",
    "    for w, c in freq.items():\n",
    "        if c >= min_count:\n",
    "            vocab.append(w)\n",
    "    vocab_dict = {w: i for i, w in enumerate(sorted(vocab))}\n",
    "    return vocab_dict\n",
    "\n",
    "vocab_dict = build_vocabulary(train_raw_tokens, min_count=2)\n",
    "print(\"Vocab size =\", len(vocab_dict))\n",
    "\n",
    "# Step 4: replace OOV with <unk> in train\n",
    "def replace_oov_with_unk(token_lists, vocab_dict):\n",
    "    new_lists = []\n",
    "    for tokens in token_lists:\n",
    "        new_tokens = []\n",
    "        for t in tokens:\n",
    "            if t not in vocab_dict:\n",
    "                new_tokens.append('<unk>')\n",
    "            else:\n",
    "                new_tokens.append(t)\n",
    "        new_lists.append(new_tokens)\n",
    "    return new_lists\n",
    "\n",
    "train_tokens = replace_oov_with_unk(train_raw_tokens, vocab_dict)\n",
    "\n",
    "# For validation\n",
    "val_raw_tokens = []\n",
    "for txt in val_texts:\n",
    "    val_raw_tokens.append(simple_tokenize(txt))\n",
    "val_raw_tokens = add_start_end_tokens(val_raw_tokens)\n",
    "val_tokens = replace_oov_with_unk(val_raw_tokens, vocab_dict)\n",
    "\n",
    "V = len(vocab_dict)\n",
    "\n",
    "\n",
    "# For test\n",
    "test_raw_tokens = []\n",
    "for txt in test_texts:\n",
    "    raw = simple_tokenize(txt)\n",
    "    test_raw_tokens.append(raw)\n",
    "\n",
    "test_raw_tokens = add_start_end_tokens(test_raw_tokens)\n",
    "test_tokens = replace_oov_with_unk(test_raw_tokens, vocab_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Part 1: N-gram Language Models\n",
    "\n",
    "-   `unigram_counts[(w,)]` is the count of word `w` in the entire training data.\n",
    "\n",
    "-   `bigram_counts[(w1,w2)]` is the count of the pair `(w1, w2)`.\n",
    "\n",
    "-   `trigram_counts[(w1,w2,w3)]` is the count of the triple `(w1, w2, w3)`.     \n",
    "\n",
    "\n",
    "p(w2|w1) = count(w1,w2)/count(w1) and  . . .   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Step 5: re-count n-grams with the final (unk-ified) train tokens\n",
    "def count_ngrams(token_lists, n):\n",
    "    ngram_counts = defaultdict(int)\n",
    "    for tokens in token_lists:\n",
    "        if len(tokens) < n: \n",
    "            continue\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "unigram_counts = count_ngrams(train_tokens, 1)\n",
    "bigram_counts  = count_ngrams(train_tokens, 2)\n",
    "trigram_counts = count_ngrams(train_tokens, 3)\n",
    "\n",
    "zerogram_total = sum(unigram_counts.values())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Smoothing\n",
    "\n",
    "\n",
    "-   **Dirichlet smoothing** for the unigram model (equivalent to additive smoothing):\n",
    "\n",
    "    ![Dirichlet smoothing](./images/Dirichlet%20smoothing.png)\n",
    "\n",
    "\n",
    "    where `N` is the total number of words in the training data, `V` is the size of the vocabulary, and `α` is a hyperparameter that can be tuned on a validation set.\n",
    "\n",
    "\n",
    "-   **Kneser-Ney smoothing** for the bigram and trigram cases. A simplified Kneser-Ney for bigrams:\n",
    "\n",
    "    ![Kneser-Ney smoothing](./images/Kneser-Ney%20smoothing.png)\n",
    "\n",
    "\n",
    "\n",
    "For **trigram** Kneser-Ney, a similar hierarchical formula:\n",
    "\n",
    "![trigram smoothing](./images/trigram%20Kneser-Ney.png)\n",
    "\n",
    "\n",
    "**Note**: The discount `d` (sometimes `D`) is often learned or tuned on validation data. Typically it's in `(0,1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, n, vocab_dict, ngram_counts, lower_order_counts=None, total_unigrams=None, discount=0.75, alpha=0.1):\n",
    "        \"\"\"\n",
    "        n = 1,2,3\n",
    "        vocab_dict: dictionary mapping tokens -> ids (optional usage)\n",
    "        ngram_counts: dict for n-gram counts\n",
    "        lower_order_counts: dict for (n-1)-gram counts (for n>=2)\n",
    "        total_unigrams: total count of all unigrams (for unigrams normalizing)\n",
    "        discount: for Kneser-Ney\n",
    "        alpha: for Dirichlet smoothing in the unigram case\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.vocab_dict = vocab_dict\n",
    "        self.ngram_counts = ngram_counts\n",
    "        self.lower_order_counts = lower_order_counts\n",
    "        self.total_unigrams = total_unigrams\n",
    "        self.discount = discount\n",
    "        self.alpha = alpha\n",
    "        \n",
    "\n",
    "        if n == 2:\n",
    "            # how many distinct contexts lead to each word w2\n",
    "            self.num_preceding_contexts = defaultdict(int)\n",
    "            for (w1, w2), c in ngram_counts.items():\n",
    "                self.num_preceding_contexts[w2] += 1\n",
    "            self.total_bigram_types = len(ngram_counts)\n",
    "            \n",
    "            self.num_bigrams_starting_with = defaultdict(int)\n",
    "            for (w1, w2), count in ngram_counts.items():\n",
    "                self.num_bigrams_starting_with[w1] += 1\n",
    "\n",
    "\n",
    "        elif n == 3:\n",
    "            # We still need the distinct followers data for the trigram discount:\n",
    "            self.num_distinct_following = defaultdict(set)\n",
    "            for (w1, w2, w3), c in ngram_counts.items():\n",
    "                self.num_distinct_following[(w1, w2)].add(w3)\n",
    "            self.total_trigram_types = len(ngram_counts)\n",
    "\n",
    "\n",
    "            self.num_preceding_contexts = defaultdict(int)\n",
    "            for (x1, x2), c in lower_order_counts.items():\n",
    "                self.num_preceding_contexts[x2] += 1\n",
    "            self.total_bigram_types = len(lower_order_counts)\n",
    "\n",
    "            self.num_bigrams_starting_with = defaultdict(int)\n",
    "            for (x1, x2), c in lower_order_counts.items():\n",
    "                self.num_bigrams_starting_with[x1] += 1\n",
    "\n",
    "\n",
    "    # Dirichlet (add-alpha) smoothing\n",
    "    def unigram_prob(self, w):\n",
    "        count_w = self.ngram_counts.get((w,), 0)\n",
    "        numerator = count_w + self.alpha\n",
    "        denominator = self.total_unigrams + self.alpha * len(self.vocab_dict)\n",
    "        return numerator / denominator\n",
    "\n",
    "\n",
    "    # Kneser-Ney for bigrams\n",
    "    def bigram_prob_kn(self, w1, w2):\n",
    "        # P(w2|w1) = max(count(...) - d, 0) / count(w1) + lambda(...) * P_continuation(w2)\n",
    "        \n",
    "        bigram_count = self.ngram_counts.get((w1, w2), 0)\n",
    "        unigram_count = self.lower_order_counts.get((w1,), 0) \n",
    "        \n",
    "        main_term = max(bigram_count - self.discount, 0.0) / (unigram_count if unigram_count > 0 else 1)\n",
    "        \n",
    "        lambda_w1 = (self.discount * self.num_bigrams_starting_with[w1]) / (unigram_count if unigram_count > 0 else 1)\n",
    "        \n",
    "        cont_prob = self.num_preceding_contexts[w2] / self.total_bigram_types\n",
    "        \n",
    "        return main_term + lambda_w1 * cont_prob\n",
    "\n",
    "\n",
    "    # Kneser-Ney for trigram \n",
    "    def trigram_prob_kn(self, w1, w2, w3):\n",
    "        # P(w3|w1,w2) = max(count(...) - d, 0) / count(w1,w2) + lambda(...) * bigram_prob_kn(w2,w3)\n",
    "        \n",
    "        tri_count  = self.ngram_counts.get((w1, w2, w3), 0)\n",
    "        bi_count   = self.lower_order_counts.get((w1, w2), 0)\n",
    "        main_term  = max(tri_count - self.discount, 0.0) / (bi_count if bi_count > 0 else 1)\n",
    "        \n",
    "        num_distinct = len(self.num_distinct_following[(w1, w2)])\n",
    "        lambda_w1_w2 = (self.discount * num_distinct / (bi_count if bi_count > 0 else 1))\n",
    "        \n",
    "        return main_term + lambda_w1_w2 * self.bigram_prob_kn(w2, w3)\n",
    "\n",
    "\n",
    "    def ngram_prob(self, context, w):\n",
    "        \"\"\"\n",
    "        context: tuple of length n-1\n",
    "        w: the next word\n",
    "        Return the smoothed probability of word w given context (depending on self.n).\n",
    "        \"\"\"\n",
    "        if self.n == 1:\n",
    "            return self.unigram_prob(w)\n",
    "        elif self.n == 2:\n",
    "            w1 = context[-1]\n",
    "            return self.bigram_prob_kn(w1, w)\n",
    "        else:\n",
    "            w1, w2 = context[-2], context[-1]\n",
    "            return self.trigram_prob_kn(w1, w2, w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Choosing/Optimizing Hyperparameters on Validation\n",
    "\n",
    "\n",
    "-   `alpha` for your **unigram** Dirichlet smoothing,\n",
    "\n",
    "-   `discount` for your Kneser-Ney in bigram/trigram.   \n",
    "\n",
    "-   `perplexity` is a common metric in language modeling that indicates how well a model predicts a sample of text. A lower perplexity means the model is more \"certain\" (or places higher probability) on the actual sequence of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Unigram] best alpha = 0.01, val perplexity = 1308.1129901488932\n",
      "[Bigram] best discount = 0.75, val perplexity = 204.19341350665897\n",
      "[Trigram] best discount = 0.75, val perplexity = 6082.749435511439\n",
      "\n",
      "Train perplexities: 1651.5449061453303 73.5026099910924 3.9270501360394254\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def perplexity(model, token_lists):\n",
    "    \"\"\"\n",
    "    Computes perplexity for any N-gram model (unigram, bigram, trigram).\n",
    "    \"\"\"\n",
    "    log_prob_sum = 0.0\n",
    "    total_count  = 0\n",
    "\n",
    "    n = model.n\n",
    "    for tokens in token_lists:\n",
    "        if len(tokens) < n:\n",
    "            continue\n",
    "\n",
    "        for i in range(n-1, len(tokens)):\n",
    "            context = tuple(tokens[i - (n - 1) : i])\n",
    "            w = tokens[i]\n",
    "            p = model.ngram_prob(context, w)\n",
    "            log_prob_sum += math.log(max(p, 1e-15))\n",
    "            total_count += 1\n",
    "\n",
    "    if total_count == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    return math.exp(-log_prob_sum / total_count)\n",
    "\n",
    "\n",
    "\n",
    "def build_ngram_model(\n",
    "    n,\n",
    "    vocab_dict,\n",
    "    unigram_counts,\n",
    "    bigram_counts,\n",
    "    trigram_counts,\n",
    "    total_unigrams,\n",
    "    alpha=0.1,\n",
    "    discount=0.75,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates and returns an NGramModel of order n with the specified\n",
    "    hyperparameters alpha (for unigram Dirichlet) and discount (for\n",
    "    Kneser-Ney in bigrams/trigrams).\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        # For unigrams, alpha smoothing\n",
    "        return NGramModel(\n",
    "            n=1,\n",
    "            vocab_dict=vocab_dict,\n",
    "            ngram_counts=unigram_counts,\n",
    "            total_unigrams=total_unigrams,\n",
    "            alpha=alpha,\n",
    "        )\n",
    "    elif n == 2:\n",
    "        # For bigrams, discount for Kneser-Ney\n",
    "        return NGramModel(\n",
    "            n=2,\n",
    "            vocab_dict=vocab_dict,\n",
    "            ngram_counts=bigram_counts,\n",
    "            lower_order_counts=unigram_counts,\n",
    "            total_unigrams=total_unigrams,\n",
    "            discount=discount,\n",
    "        )\n",
    "    else:\n",
    "        # trigram\n",
    "        return NGramModel(\n",
    "            n=3,\n",
    "            vocab_dict=vocab_dict,\n",
    "            ngram_counts=trigram_counts,\n",
    "            lower_order_counts=bigram_counts,\n",
    "            total_unigrams=total_unigrams,\n",
    "            discount=discount,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# grid search over possible α and discount values\n",
    "possible_alphas    = [0.01, 0.1, 0.5, 0.9]\n",
    "possible_discounts = [0.1, 0.5, 0.6, 0.75]\n",
    "\n",
    "\n",
    "######################\n",
    "# UNIGRAM \n",
    "######################\n",
    "\n",
    "best_alpha_uni = None\n",
    "best_val_pp_uni = float('inf')\n",
    "\n",
    "for alpha in possible_alphas:\n",
    "    model_uni = build_ngram_model(\n",
    "        n=1,\n",
    "        vocab_dict=vocab_dict,\n",
    "        unigram_counts=unigram_counts,\n",
    "        bigram_counts=bigram_counts,\n",
    "        trigram_counts=trigram_counts,\n",
    "        total_unigrams=zerogram_total,\n",
    "        alpha=alpha\n",
    "        )\n",
    "    pp_val = perplexity(model_uni, val_tokens)\n",
    "    if pp_val < best_val_pp_uni:\n",
    "        best_val_pp_uni = pp_val\n",
    "        best_alpha_uni  = alpha\n",
    "\n",
    "print(f\"[Unigram] best alpha = {best_alpha_uni}, val perplexity = {best_val_pp_uni}\")\n",
    "\n",
    "\n",
    "######################\n",
    "# BIGRAM \n",
    "######################\n",
    "\n",
    "best_discount_bi = None\n",
    "best_val_pp_bi   = float('inf')\n",
    "\n",
    "for d in possible_discounts:\n",
    "    model_bi = build_ngram_model(\n",
    "        n=2,\n",
    "        vocab_dict=vocab_dict,\n",
    "        unigram_counts=unigram_counts,\n",
    "        bigram_counts=bigram_counts,\n",
    "        trigram_counts=trigram_counts,\n",
    "        total_unigrams=zerogram_total,\n",
    "        discount=d\n",
    "    )\n",
    "    pp_val = perplexity(model_bi, val_tokens)\n",
    "    if pp_val < best_val_pp_bi:\n",
    "        best_val_pp_bi = pp_val\n",
    "        best_discount_bi = d\n",
    "\n",
    "print(f\"[Bigram] best discount = {best_discount_bi}, val perplexity = {best_val_pp_bi}\")\n",
    "\n",
    "\n",
    "######################\n",
    "# TRIGRAM \n",
    "######################\n",
    "\n",
    "best_discount_tri = None\n",
    "best_val_pp_tri   = float('inf')\n",
    "\n",
    "for d in possible_discounts:\n",
    "    model_tri = build_ngram_model(\n",
    "        n=3,\n",
    "        vocab_dict=vocab_dict,\n",
    "        unigram_counts=unigram_counts,\n",
    "        bigram_counts=bigram_counts,\n",
    "        trigram_counts=trigram_counts,\n",
    "        total_unigrams=zerogram_total,\n",
    "        discount=d\n",
    "    )\n",
    "    pp_val = perplexity(model_tri, val_tokens)\n",
    "    if pp_val < best_val_pp_tri:\n",
    "        best_val_pp_tri = pp_val\n",
    "        best_discount_tri = d\n",
    "\n",
    "print(f\"[Trigram] best discount = {best_discount_tri}, val perplexity = {best_val_pp_tri}\")\n",
    "print()\n",
    "\n",
    "ppl_train_uni = perplexity(model_uni, train_tokens)\n",
    "ppl_train_bi  = perplexity(model_bi,  train_tokens)\n",
    "ppl_train_tri = perplexity(model_tri, train_tokens)\n",
    "print(\"Train perplexities:\", ppl_train_uni, ppl_train_bi, ppl_train_tri)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Evaluate on Test Set\n",
    "\n",
    "After determining the best hyperparameters, we finalize our model then measure perplexity on `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST PERPLEXITIES ===\n",
      "Unigram model perplexity: 1312.1047027555612\n",
      "Bigram  model perplexity: 204.06683231216311\n",
      "Trigram model perplexity: 5738.741642152637\n",
      "\n",
      "Bigram is best on test set.\n"
     ]
    }
   ],
   "source": [
    "# Rebuild best models using chosen hyperparams\n",
    "\n",
    "best_uni_model = build_ngram_model(\n",
    "    n=1,\n",
    "    vocab_dict=vocab_dict,\n",
    "    unigram_counts=unigram_counts,\n",
    "    bigram_counts=bigram_counts,\n",
    "    trigram_counts=trigram_counts,\n",
    "    total_unigrams=zerogram_total,\n",
    "    alpha=best_alpha_uni\n",
    ")\n",
    "\n",
    "best_bi_model = build_ngram_model(\n",
    "    n=2,\n",
    "    vocab_dict=vocab_dict,\n",
    "    unigram_counts=unigram_counts,\n",
    "    bigram_counts=bigram_counts,\n",
    "    trigram_counts=trigram_counts,\n",
    "    total_unigrams=zerogram_total,\n",
    "    discount=best_discount_bi \n",
    ")\n",
    "\n",
    "best_tri_model = build_ngram_model(\n",
    "    n=3,\n",
    "    vocab_dict=vocab_dict,\n",
    "    unigram_counts=unigram_counts,\n",
    "    bigram_counts=bigram_counts,\n",
    "    trigram_counts=trigram_counts,\n",
    "    total_unigrams=zerogram_total,\n",
    "    discount=best_discount_tri\n",
    ")\n",
    "\n",
    "# compute perplexities on the test set:\n",
    "test_pp_uni = perplexity(best_uni_model, test_tokens)\n",
    "test_pp_bi  = perplexity(best_bi_model,  test_tokens)\n",
    "test_pp_tri = perplexity(best_tri_model, test_tokens)\n",
    "\n",
    "print(\"\\n=== TEST PERPLEXITIES ===\")\n",
    "print(f\"Unigram model perplexity: {test_pp_uni}\")\n",
    "print(f\"Bigram  model perplexity: {test_pp_bi}\")\n",
    "print(f\"Trigram model perplexity: {test_pp_tri}\")\n",
    "print()\n",
    "\n",
    "lowest_pp = min(test_pp_uni, test_pp_bi, test_pp_tri)\n",
    "if lowest_pp == test_pp_uni:\n",
    "    print(\"Unigram is best on test set.\")\n",
    "elif lowest_pp == test_pp_bi:\n",
    "    print(\"Bigram is best on test set.\")\n",
    "else:\n",
    "    print(\"Trigram is best on test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Part 2: Feed-Forward Neural Network\n",
    "--------------------------------------\n",
    "\n",
    "Here, you must:\n",
    "\n",
    "1.  Represent the input context (unigram, bigram, trigram) in some way. Two common ways:\n",
    "\n",
    "    -   **One-hot** of the context words. (If `n=3` (trigram), you have 2 words in context, each one-hot if the vocabulary is not too large.)\n",
    "\n",
    "    -   **Train an embedding** from scratch: each word has a small embedding vector that you learn simultaneously (similar to word2vec).\n",
    "\n",
    "2.  Build a basic feed-forward network with at least one hidden layer.\n",
    "\n",
    "3.  Train it to **predict the next word** from the context.\n",
    "\n",
    "4.  Tune learning rate (and possibly hidden-layer size) using the validation set.\n",
    "\n",
    "5.  Evaluate perplexity on the test set, same formula:\n",
    "\n",
    "    ![perplexity](./images/perplexity.png)\n",
    "\n",
    "    where `N` is the total number of words in the test set and `p(w_i|w_{i-1},w_{i-2})` is the probability assigned by your model.\n",
    "\n",
    "> **Note**: Usually, you'd use PyTorch/TensorFlow to handle all details. Since **libraries are not allowed** for the fundamental logic, you must implement your own forward/backprop. **(You may or may not be allowed to use `numpy` for basic matrix multiplies.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Feed-Forward Network\n",
    "##############################################\n",
    "\n",
    "class SimpleFeedForwardNN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim=50,\n",
    "        hidden_size=100,\n",
    "        context_size=2,\n",
    "        learning_rate=0.01,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        vocab_size: total number of words in the vocabulary\n",
    "        embed_dim: dimension of embeddings for each word\n",
    "        hidden_size: number of hidden units\n",
    "        context_size: how many words form the context (2 for trigram)\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.context_size = context_size\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # Each row corresponds to embedding of one word in the vocabulary [vocab_size, embed_dim]\n",
    "        self.embeddings = [\n",
    "            [(random.random() - 0.5) * 0.01 for _ in range(embed_dim)]\n",
    "            for _ in range(vocab_size)\n",
    "        ]\n",
    "\n",
    "        # input to the hidden layer is a concatenation of all context word embeddings\n",
    "        self.W1 = [\n",
    "            [(random.random() - 0.5) * 0.01 for _ in range(hidden_size)]\n",
    "            for _ in range(context_size * embed_dim)\n",
    "        ]\n",
    "        \n",
    "        # Output layer \n",
    "        # Each row corresponds to one hidden neuron, and each column corresponds to an output dimension\n",
    "        self.W2 = [\n",
    "            [(random.random() - 0.5) * 0.01 for _ in range(vocab_size)]\n",
    "            for _ in range(hidden_size)\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, context_word_ids):\n",
    "        \n",
    "        # 1) embed each context word, then concatenate\n",
    "        x = []\n",
    "        for wid in context_word_ids:\n",
    "            x.extend(self.embeddings[wid])\n",
    "\n",
    "        # 2) Hidden layer computation (input @ hidden) \n",
    "        hidden = [0] * self.hidden_size\n",
    "        for j in range(self.hidden_size):\n",
    "            s = 0\n",
    "            for i in range(len(x)):\n",
    "                s += x[i] * self.W1[i][j]\n",
    "            hidden[j] = math.tanh(s)\n",
    "\n",
    "        # 3) output logits =  (hidden @ output)\n",
    "        # unnormalized scores for each word in the vocabulary\n",
    "        logits = [0] * self.vocab_size\n",
    "        for k in range(self.vocab_size):\n",
    "            s = 0\n",
    "            for j in range(self.hidden_size):\n",
    "                s += hidden[j] * self.W2[j][k]\n",
    "            logits[k] = s\n",
    "\n",
    "        # 4) softmax\n",
    "        max_logit = max(logits)\n",
    "        exps = [math.exp(log - max_logit) for log in logits]\n",
    "        sum_exps = sum(exps)\n",
    "        probs = [e / sum_exps for e in exps]\n",
    "\n",
    "        # input vector x, the hidden layer output and (model’s prediction distribution over the vocabulary) \n",
    "        return x, hidden, probs\n",
    "\n",
    "\n",
    "    def backward(self, x, hidden, probs, target_word_id, context_word_ids):\n",
    "        # 1) derivative wrt logits = (pred - actual)\n",
    "        dlogits = [p for p in probs]\n",
    "        dlogits[target_word_id] -= 1.0\n",
    "\n",
    "        # 2) update W2 using dlogits * hidden\n",
    "        dW2 = [[0]*self.vocab_size for _ in range(self.hidden_size)]\n",
    "        for j in range(self.hidden_size):\n",
    "            for k in range(self.vocab_size):\n",
    "                dW2[j][k] = hidden[j] * dlogits[k]\n",
    "\n",
    "        # 3) derivative wrt hidden\n",
    "        dhidden = [0] * self.hidden_size\n",
    "        for j in range(self.hidden_size):\n",
    "            s = 0\n",
    "            for k in range(self.vocab_size):\n",
    "                s += dlogits[k] * self.W2[j][k]\n",
    "            dhidden[j] = s*(1 - hidden[j] * hidden[j])  # derivative of tanh\n",
    "\n",
    "        # 4) update W1\n",
    "        dW1 = [[0]*self.hidden_size for _ in range(len(x))]\n",
    "        for i in range(len(x)):\n",
    "            for j in range(self.hidden_size):\n",
    "                dW1[i][j] = x[i]*dhidden[j]\n",
    "\n",
    "        # 5) apply gradient updates to W2\n",
    "        for j in range(self.hidden_size):\n",
    "            for k in range(self.vocab_size):\n",
    "                self.W2[j][k] -= self.lr*dW2[j][k]\n",
    "\n",
    "        # 6) apply gradient updates to W1\n",
    "        for i in range(len(x)):\n",
    "            for j in range(self.hidden_size):\n",
    "                self.W1[i][j] -= self.lr*dW1[i][j]\n",
    "\n",
    "        # 7) derivative wrt embeddings\n",
    "        dembeddings = [[0]*self.embed_dim for _ in range(self.context_size)]\n",
    "        for c_i in range(self.context_size):\n",
    "            for j in range(self.hidden_size):\n",
    "                for e_i in range(self.embed_dim):\n",
    "                    i_global = c_i*self.embed_dim + e_i\n",
    "                    dembeddings[c_i][e_i] += dhidden[j]*self.W1[i_global][j]\n",
    "\n",
    "        # update embeddings\n",
    "        for c_i, wid in enumerate(context_word_ids):\n",
    "            for e_i in range(self.embed_dim):\n",
    "                self.embeddings[wid][e_i] -= self.lr*dembeddings[c_i][e_i]\n",
    "\n",
    "    def train_on_example(self, context_word_ids, target_word_id):\n",
    "        x, hidden, probs = self.forward(context_word_ids)\n",
    "        self.backward(x, hidden, probs, target_word_id, context_word_ids)\n",
    "\n",
    "    def predict_next_word_prob(self, context_word_ids):\n",
    "        _, _, probs = self.forward(context_word_ids)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation and paramter tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "##########################################################\n",
    "# Convert sequences of tokens into pairs (context, target)\n",
    "##########################################################\n",
    "\n",
    "def build_context_target_pairs(token_lists, n, vocab_dict):\n",
    "    \"\"\"\n",
    "    For n=1 (unigram model), context_size=0 => we can feed an empty context or\n",
    "    a special <NULL> token to the network.\n",
    "\n",
    "    For n=2 (bigram), context_size=1 => each training example is (w_{i-1}, w_i).\n",
    "    For n=3 (trigram), context_size=2 => (w_{i-2}, w_{i-1}, w_i).\n",
    "    etc.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    context_size = n - 1\n",
    "\n",
    "    for tokens in tqdm(token_lists, desc=\"Building context-target pairs\", unit=\"sentence\"):\n",
    "        if len(tokens) < n:\n",
    "            continue\n",
    "        for i in range(context_size, len(tokens)):\n",
    "            context_tokens = tokens[i - context_size : i]\n",
    "            target_token = tokens[i]\n",
    "            context_ids = [vocab_dict.get(t, 0) for t in context_tokens]\n",
    "            target_id = vocab_dict.get(target_token, 0)\n",
    "            pairs.append((context_ids, target_id))\n",
    "            \n",
    "    return pairs\n",
    "\n",
    "\n",
    "##########################################\n",
    "# 3) Perplexity for the feed-forward model\n",
    "##########################################\n",
    "\n",
    "def perplexity_ffnn(nn_model, pairs):\n",
    "    \"\"\"\n",
    "    Calculate perplexity from (context, target) pairs directly.\n",
    "    sum -log p(target|context) across all pairs, then exponentiate.\n",
    "    \"\"\"\n",
    "    log_prob_sum = 0.0\n",
    "    total_count = 0\n",
    "\n",
    "    for (ctx_ids, tgt_id) in tqdm(pairs, desc=\"Calculating perplexity\", unit=\"pair\"):\n",
    "        probs = nn_model.predict_next_word_prob(ctx_ids)\n",
    "        p = probs[tgt_id]\n",
    "        if p <= 0:\n",
    "            log_prob_sum += -999999\n",
    "        else:\n",
    "            log_prob_sum += math.log(p)\n",
    "        total_count += 1\n",
    "        \n",
    "\n",
    "    if total_count == 0:\n",
    "        return float\n",
    "    (\"inf\")\n",
    "\n",
    "    return math.exp(-log_prob_sum / total_count)\n",
    "\n",
    "\n",
    "############################################\n",
    "# tries different learning rates, picks best \n",
    "############################################\n",
    "\n",
    "def train_ffnn_for_ngram(\n",
    "    n,\n",
    "    train_tokens,\n",
    "    val_tokens,\n",
    "    vocab_dict,\n",
    "    embed_dim=50,\n",
    "    hidden_size=100,\n",
    "    learning_rates=[0.01, 0.1, 0.5, 1.0],\n",
    "    epochs=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    For a given n (1,2,3):\n",
    "    1) Build (context, target) pairs for train, val\n",
    "    2) Loop over possible learning_rates\n",
    "    3) For each LR, train a new model for epochs\n",
    "    4) Track the best validation perplexity\n",
    "    5) Return the best model (or its LR)\n",
    "    \"\"\"\n",
    "\n",
    "    train_pairs = build_context_target_pairs(train_tokens, n, vocab_dict)\n",
    "    val_pairs = build_context_target_pairs(val_tokens, n, vocab_dict)\n",
    "\n",
    "    best_lr = None\n",
    "    best_val_pp = float(\"inf\")\n",
    "    best_model = None\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        context_size = max(n - 1, 1)\n",
    "        nn = SimpleFeedForwardNN(\n",
    "            vocab_size=len(vocab_dict),\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            context_size=context_size,\n",
    "            learning_rate=lr,\n",
    "        )\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            random.shuffle(train_pairs)\n",
    "            for (ctx, tgt) in tqdm(train_pairs, desc=f\"Epoch {ep+1}/{epochs}\", unit=\"batch\"):\n",
    "                nn.train_on_example(ctx, tgt)\n",
    "\n",
    "            val_pp = perplexity_ffnn(nn, val_pairs)\n",
    "\n",
    "        final_val_pp = perplexity_ffnn(nn, val_pairs)\n",
    "        if final_val_pp < best_val_pp:\n",
    "            best_val_pp = final_val_pp\n",
    "            best_lr = lr\n",
    "            best_model = nn\n",
    "\n",
    "\n",
    "    print(f\"** For n={n}, best LR={best_lr}, perplexity={best_val_pp}\")\n",
    "    \n",
    "    return best_model  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Best Model from Part 1 or Part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building context-target pairs: 100%|██████████| 855/855 [00:00<00:00, 1378.33sentence/s]\n",
      "Building context-target pairs: 100%|██████████| 106/106 [00:00<00:00, 6846.68sentence/s]\n",
      "Epoch 1/1: 100%|██████████| 109797/109797 [56:31<00:00, 32.37batch/s] \n",
      "Calculating perplexity: 100%|██████████| 13136/13136 [03:42<00:00, 59.07pair/s]\n",
      "Calculating perplexity: 100%|██████████| 13136/13136 [03:45<00:00, 58.19pair/s]\n",
      "Epoch 1/1: 100%|██████████| 109797/109797 [54:07<00:00, 33.81batch/s] \n",
      "Calculating perplexity: 100%|██████████| 13136/13136 [03:53<00:00, 56.35pair/s]\n",
      "Calculating perplexity: 100%|██████████| 13136/13136 [04:02<00:00, 54.09pair/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** For n=1, best LR=0.01, perplexity=34388.00000004098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building context-target pairs: 100%|██████████| 855/855 [00:00<00:00, 1500.49sentence/s]\n",
      "Building context-target pairs: 100%|██████████| 106/106 [00:00<00:00, 8669.97sentence/s]\n",
      "Epoch 1/1: 100%|██████████| 108942/108942 [1:05:32<00:00, 27.71batch/s]\n",
      "Calculating perplexity: 100%|██████████| 13030/13030 [04:46<00:00, 45.48pair/s]\n",
      "Calculating perplexity: 100%|██████████| 13030/13030 [04:45<00:00, 45.68pair/s]\n",
      "Epoch 1/1: 100%|██████████| 108942/108942 [1:07:34<00:00, 26.87batch/s]\n",
      "Calculating perplexity: 100%|██████████| 13030/13030 [04:57<00:00, 43.73pair/s]\n",
      "Calculating perplexity: 100%|██████████| 13030/13030 [04:59<00:00, 43.56pair/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** For n=2, best LR=0.01, perplexity=34387.999931053615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building context-target pairs: 100%|██████████| 855/855 [00:00<00:00, 6024.06sentence/s]\n",
      "Building context-target pairs: 100%|██████████| 106/106 [00:00<00:00, 6958.12sentence/s]\n",
      "Epoch 1/1: 100%|██████████| 108087/108087 [1:08:26<00:00, 26.32batch/s]\n",
      "Calculating perplexity: 100%|██████████| 12924/12924 [04:54<00:00, 43.88pair/s]\n",
      "Calculating perplexity: 100%|██████████| 12924/12924 [04:55<00:00, 43.68pair/s]\n",
      "Epoch 1/1: 100%|██████████| 108087/108087 [1:09:48<00:00, 25.81batch/s]\n",
      "Calculating perplexity: 100%|██████████| 12924/12924 [04:58<00:00, 43.36pair/s]\n",
      "Calculating perplexity: 100%|██████████| 12924/12924 [05:01<00:00, 42.86pair/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** For n=3, best LR=0.1, perplexity=34387.99996559879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building context-target pairs: 100%|██████████| 1069/1069 [00:01<00:00, 974.57sentence/s]\n",
      "Building context-target pairs: 100%|██████████| 1069/1069 [00:00<00:00, 7733.48sentence/s]\n",
      "Building context-target pairs: 100%|██████████| 1069/1069 [00:00<00:00, 1607.43sentence/s]\n",
      "Calculating perplexity: 100%|██████████| 141528/141528 [47:41<00:00, 49.45pair/s] \n",
      "Calculating perplexity: 100%|██████████| 140459/140459 [50:11<00:00, 46.65pair/s] \n",
      "Calculating perplexity: 100%|██████████| 139390/139390 [49:08<00:00, 47.27pair/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST PERPLEXITIES (Feed-Forward) ===\n",
      "Unigram   => 34388.00000038776\n",
      "Bigram    => 34387.99991768191\n",
      "Trigram   => 34387.999920531416\n",
      "\n",
      "\n",
      "Now comparing with best n-gram results from Section 1:\n",
      "Unigram n-gram perplexity:  1312.1047027555612\n",
      "Bigram n-gram perplexity:   204.06683231216311\n",
      "Trigram n-gram perplexity:  5738.741642152637\n",
      "\n",
      "\n",
      "Feed-Forward Neural Network:\n",
      "Unigram n-gram perplexity:  1312.1047027555612\n",
      "Bigram n-gram perplexity:   204.06683231216311\n",
      "Trigram n-gram perplexity:  5738.741642152637\n"
     ]
    }
   ],
   "source": [
    "# Reduce the training and validation data size for faster debugging\n",
    "# train_tokens_sample = train_tokens[:int(0.1 * len(train_tokens))]  # Use 10% of the data\n",
    "# val_tokens_sample = val_tokens[:int(0.1 * len(val_tokens))]\n",
    "\n",
    "train_tokens_sample = train_tokens[:int(1 * len(train_tokens))]\n",
    "val_tokens_sample = val_tokens[:int(1 * len(val_tokens))]\n",
    "\n",
    "    \n",
    "# (1) Train a unigram feed-forward model\n",
    "ffnn_unigram = train_ffnn_for_ngram(\n",
    "    n=1,\n",
    "    train_tokens=train_tokens_sample,\n",
    "    val_tokens=val_tokens_sample,\n",
    "    vocab_dict=vocab_dict,\n",
    "    embed_dim=50,\n",
    "    hidden_size=100,\n",
    "    learning_rates=[0.001, 0.01, 0.1, 0.5],\n",
    "    epochs=2\n",
    ")\n",
    "\n",
    "# (2) Train a BIGRAM feed-forward model\n",
    "ffnn_bigram = train_ffnn_for_ngram(\n",
    "    n=2,\n",
    "    train_tokens=train_tokens_sample,\n",
    "    val_tokens=val_tokens_sample,\n",
    "    vocab_dict=vocab_dict,\n",
    "    embed_dim=50,\n",
    "    hidden_size=100,\n",
    "    learning_rates=[0.001, 0.01, 0.1, 0.5],\n",
    "    epochs=2\n",
    ")\n",
    "\n",
    "# (3) Train a TRIGRAM feed-forward model\n",
    "ffnn_trigram = train_ffnn_for_ngram(\n",
    "    n=3,\n",
    "    train_tokens=train_tokens_sample,\n",
    "    val_tokens=val_tokens_sample,\n",
    "    vocab_dict=vocab_dict,\n",
    "    embed_dim=50,\n",
    "    hidden_size=100,\n",
    "    learning_rates=[0.001, 0.01, 0.1, 0.5],\n",
    "    epochs=2\n",
    ")\n",
    "\n",
    "# Evaluate all on TEST\n",
    "test_pairs_unigram = build_context_target_pairs(test_tokens, 1, vocab_dict)\n",
    "test_pairs_bigram  = build_context_target_pairs(test_tokens, 2, vocab_dict)\n",
    "test_pairs_trigram = build_context_target_pairs(test_tokens, 3, vocab_dict)\n",
    "\n",
    "test_pp_uni_nn = perplexity_ffnn(ffnn_unigram,  test_pairs_unigram)\n",
    "test_pp_bi_nn  = perplexity_ffnn(ffnn_bigram,   test_pairs_bigram)\n",
    "test_pp_tri_nn = perplexity_ffnn(ffnn_trigram,  test_pairs_trigram)\n",
    "\n",
    "print(\"\\n=== TEST PERPLEXITIES (Feed-Forward) ===\")\n",
    "print(f\"Unigram   => {test_pp_uni_nn}\")\n",
    "print(f\"Bigram    => {test_pp_bi_nn}\")\n",
    "print(f\"Trigram   => {test_pp_tri_nn}\")\n",
    "print()\n",
    "\n",
    "test_pp_uni_nn = perplexity(best_uni_model, test_tokens)\n",
    "test_pp_bi_nn  = perplexity(best_bi_model,  test_tokens)\n",
    "test_pp_tri_nn = perplexity(best_tri_model, test_tokens)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nNow comparing with best n-gram results from Section 1:\")\n",
    "print(f\"Unigram n-gram perplexity:  {test_pp_uni}\")\n",
    "print(f\"Bigram n-gram perplexity:   {test_pp_bi}\")\n",
    "print(f\"Trigram n-gram perplexity:  {test_pp_tri}\")\n",
    "print()\n",
    "print(\"\\nFeed-Forward Neural Network:\")\n",
    "print(f\"Unigram n-gram perplexity:  {test_pp_uni_nn}\")\n",
    "print(f\"Bigram n-gram perplexity:   {test_pp_bi_nn}\")\n",
    "print(f\"Trigram n-gram perplexity:  {test_pp_tri_nn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 5.1 Best Model from Part 1 or Part 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=> The best overall model is: Bigram N-gram with perplexity = 204.06683231216311\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_perplexities = {\n",
    "    \"Unigram N-gram\":  (test_pp_uni,     best_uni_model),\n",
    "    \"Bigram N-gram\":   (test_pp_bi,      best_bi_model),\n",
    "    \"Trigram N-gram\":  (test_pp_tri,     best_tri_model),\n",
    "    \"Unigram NN\":      (test_pp_uni_nn,  ffnn_unigram),\n",
    "    \"Bigram NN\":       (test_pp_bi_nn,   ffnn_bigram),\n",
    "    \"Trigram NN\":      (test_pp_tri_nn,  ffnn_trigram),\n",
    "}\n",
    "\n",
    "best_model_name = None\n",
    "best_pp = float('inf')\n",
    "best_model_obj = None\n",
    "\n",
    "for name, (pp, model_obj) in model_perplexities.items():\n",
    "    if pp < best_pp:\n",
    "        best_pp = pp\n",
    "        best_model_name = name\n",
    "        best_model_obj  = model_obj\n",
    "\n",
    "print(f\"\\n=> The best overall model is: {best_model_name} with perplexity = {best_pp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 25.52word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 22.85word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 18.22word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.31word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.52word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.47word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.92word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 18.44word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 18.21word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.37word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.55word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.00word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.30word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.51word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.48word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.25word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.27word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.31word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.29word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 18.77word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.00word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 18.47word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.56word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 16.47word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.49word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 18.37word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 18.81word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 18.01word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 17.17word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.24word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.89word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.99word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.41word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 18.75word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.07word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.73word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.46word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.97word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.61word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.19word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.14word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.66word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.19word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 22.20word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.88word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.84word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.60word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.27word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.09word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.09word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.23word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.61word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.56word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.72word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.43word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.94word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.59word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 18.87word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.34word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 14.99word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.55word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.24word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.70word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 18.75word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.60word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.38word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.49word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.45word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.14word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.72word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.77word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.53word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.57word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.62word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.68word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.75word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.19word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.68word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.07word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.55word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.64word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.97word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.81word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.00word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.78word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.43word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.53word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 22.73word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.65word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 23.31word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.16word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.21word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 16.85word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 21.42word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.67word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.99word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.10word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 20.20word/s]\n",
      "Predicting words: 100%|██████████| 4/4 [00:00<00:00, 19.42word/s]\n",
      "Processing rows: 99row [00:25,  3.93row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions written to sample_output.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def predict_next_word_ngram(model, tokens):\n",
    "    \"\"\"\n",
    "    Predict SINGLE next word given the last (n-1) tokens\n",
    "    Returns the predicted word\n",
    "    \"\"\"\n",
    "    n = model.n\n",
    "    context_size = n - 1\n",
    "\n",
    "    if len(tokens) < context_size:\n",
    "        context = tuple([\"<s>\"] * (context_size - len(tokens)) + tokens)\n",
    "    else:\n",
    "        context = tuple(tokens[-context_size:])\n",
    "\n",
    "    best_word = None\n",
    "    best_prob = -1\n",
    "\n",
    "    for word in model.vocab_dict.keys():\n",
    "        p = model.ngram_prob(context, word)\n",
    "        if p > best_prob:\n",
    "            best_prob = p\n",
    "            best_word = word\n",
    "            \n",
    "            \n",
    "    return best_word\n",
    "\n",
    "\n",
    "def predict_multiple_words_ngram(model, tokens, max_words=4):\n",
    "    \"\"\"\n",
    "    predict up to 'max_words' missing words to reach end of sentence.\n",
    "    For each next word, we call 'predict_next_word_ngram', then append that word.\n",
    "    Stop if we generate '</s>'\n",
    "    \"\"\"\n",
    "    predicted_words = []\n",
    "    for _ in tqdm(range(max_words), desc=\"Predicting words\", unit=\"word\"):\n",
    "        next_w = predict_next_word_ngram(model, tokens)\n",
    "        predicted_words.append(next_w)\n",
    "        tokens.append(next_w)\n",
    "        \n",
    "        if next_w == \"</s>\":\n",
    "            break\n",
    "        \n",
    "    return predicted_words\n",
    "\n",
    "\n",
    "def read_rows_from_csv(path):\n",
    "    \"\"\"\n",
    "    Read an entire CSV into a list of dictionaries.\n",
    "    \"\"\"\n",
    "    with open(path, mode=\"r\", encoding=\"utf-8\") as fin:\n",
    "        reader = csv.DictReader(fin)\n",
    "        rows = list(reader)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def generate_csv_predictions_ngram(\n",
    "    input_csv=\"sample.csv\",\n",
    "    output_csv=\"sample_output.csv\",\n",
    "    text_col_name=\"Truncated text\",\n",
    "    best_model=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Read sample.csv, use best_model to predict:\n",
    "      (1) first missing word,\n",
    "      (2) up to 4 words,\n",
    "    then write to sample_output.csv with new columns.\n",
    "    \"\"\"\n",
    "    with open(input_csv, mode=\"r\", encoding=\"utf-8\") as fin, open(\n",
    "        output_csv, mode=\"w\", encoding=\"utf-8\", newline=\"\"\n",
    "    ) as fout:\n",
    "        reader = csv.DictReader(fin)\n",
    "        fieldnames = reader.fieldnames + [\n",
    "            \"FirstPredictedWord\",\n",
    "            \"FullPrediction\",\n",
    "        ]\n",
    "        writer = csv.DictWriter(fout, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in tqdm(reader, desc=\"Processing rows\", unit=\"row\"):\n",
    "            truncated_text = row[text_col_name].strip()\n",
    "            tokens = truncated_text.split()\n",
    "\n",
    "            # 1) Predict the first missing word\n",
    "            first_word = predict_next_word_ngram(best_model, tokens)\n",
    "\n",
    "            # 2) predict up to 4 missing words\n",
    "            predicted_sequence = predict_multiple_words_ngram(\n",
    "                best_model, tokens[:], max_words=4\n",
    "            )\n",
    "\n",
    "            row[\"FirstPredictedWord\"] = first_word\n",
    "            row[\"FullPrediction\"] = \" \".join(predicted_sequence)\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Predictions written to {output_csv}.\")\n",
    "\n",
    "\n",
    "\n",
    "sample_rows = read_rows_from_csv(\"sample.csv\")\n",
    "\n",
    "generate_csv_predictions_ngram(\n",
    "    input_csv=\"sample.csv\",\n",
    "    output_csv=\"sample_output.csv\",\n",
    "    text_col_name=\"Truncated Text\",\n",
    "    best_model=best_model_obj\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
